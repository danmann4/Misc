{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e989c8e",
   "metadata": {},
   "source": [
    "# IMDB Review Sentiment Classification\n",
    "\n",
    "This is a demonstration of how to use Natural Language Processing (NLP) techniques to help us classify IMDB movie reviews as either positive or negative. This follows a similar structure to a guide by \"The PyCoach\" on Towards Data Science. \n",
    "\n",
    "We are going to use multiple supervised classification techniques in order to determine which best discerns positive/negative movie reviews using Term Frequency - Inverse Document Frequency (TF-IDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb32550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, import the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84264d9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will import the csv file containing the reviews \n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "# Print info to see what we are working with\n",
    "print(df.info())\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b845fe65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported CSV review counts:\n",
      "           review\n",
      "sentiment        \n",
      "negative    25000\n",
      "positive    25000\n",
      "\n",
      "Subset review counts:\n",
      "           review\n",
      "sentiment        \n",
      "negative     2500\n",
      "positive     2500\n"
     ]
    }
   ],
   "source": [
    "# Check if the sentiments are equally distributed\n",
    "print('Imported CSV review counts:')\n",
    "print(df.groupby(['sentiment']).count())\n",
    "\n",
    "# Luckily the sentiments are equally distributed. \n",
    "# This is important as an unbalanced training set can produce very different results from the test set. \n",
    "# It would take some extra time to process all 25000 reviews for each sentiment. \n",
    "# We are just going to make a 10% subset from which we will draw our test/train sets. \n",
    "\n",
    "print('\\nSubset review counts:')\n",
    "sample_df = df.groupby('sentiment').apply(lambda x: x.sample(frac=0.1))\n",
    "sample_df.reset_index(inplace=True, drop = True)\n",
    "print(sample_df.groupby(['sentiment']).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be05f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our train/test sets into an 80/20 ratio respectively.\n",
    "train, test = train_test_split(sample_df, test_size = 0.2, random_state = 5)\n",
    "train_x, train_y = train['review'], train['sentiment']\n",
    "test_x, test_y = test['review'], test['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "973bbbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set TF-IDF sparse matrix shape: (4000, 35311)\n",
      "Test set TF-IDF sparse matrix shape: (1000, 35311)\n"
     ]
    }
   ],
   "source": [
    "# While we could use some more NLP techniques like tokenizsation to improve our predictions, \n",
    "# lets simply start off with TF-IDF. This will create a sparse matrix of the features (words) within the reviews\n",
    "# with the approriate weights applied for their freuqency. We'll throw out the generic stop words as well. \n",
    "TfIdf = TfidfVectorizer(stop_words = 'english')\n",
    "train_TfIdf = TfIdf.fit_transform(train_x)\n",
    "test_TfIdf = TfIdf.transform(test_x)\n",
    "print(f'Training set TF-IDF sparse matrix shape: {train_TfIdf.shape}')\n",
    "print(f'Test set TF-IDF sparse matrix shape: {test_TfIdf.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d131f8e",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "Now that we have our reviews broken down into TF-IDF weighted sparse matricies, its time to make a model with the training set. \n",
    "In order to cross-validate we are going to use multiple classification models to see which fits best. \n",
    "While there are many supervised/un-supervised options, some of the most common which we will test out are:\n",
    "\n",
    "- Support Vector Machines (SVM)\n",
    "- Decision Tree Classifiers\n",
    "- Naive Bayesian Classifiers\n",
    "- Logistic Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5caf0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM fit time elapsed: 9.277125358581543s\n"
     ]
    }
   ],
   "source": [
    "svc = SVC()\n",
    "start = time.time()\n",
    "svc.fit(train_TfIdf, train_y)\n",
    "end = time.time()\n",
    "\n",
    "print(f'SVM fit time elapsed: {end - start}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc194ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression fit time elapsed: 0.2298266887664795s\n"
     ]
    }
   ],
   "source": [
    "regression = LogisticRegression()\n",
    "start = time.time()\n",
    "regression.fit(train_TfIdf, train_y)\n",
    "end = time.time()\n",
    "\n",
    "print(f'Logistic Regression fit time elapsed: {end - start}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f701a47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree fit time elapsed: 1.339094638824463s\n"
     ]
    }
   ],
   "source": [
    "Tree = DecisionTreeClassifier()\n",
    "start = time.time()\n",
    "Tree.fit(train_TfIdf, train_y)\n",
    "end = time.time()\n",
    "\n",
    "print(f'Decision Tree fit time elapsed: {end - start}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ec372e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes fit time elapsed: 2.965023994445801s\n"
     ]
    }
   ],
   "source": [
    "NB = GaussianNB()\n",
    "start = time.time()\n",
    "NB.fit(train_TfIdf.toarray(), train_y)\n",
    "end = time.time()\n",
    "\n",
    "print(f'Naive Bayes fit time elapsed: {end - start}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7cb7b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Score: 0.86\n",
      "Decision Tree Score: 0.674\n",
      "Naive Bayes Score: 0.611\n",
      "Logistic-regression: 0.862\n"
     ]
    }
   ],
   "source": [
    "# Lets check on the mean accuracy value of each model when the test set is fed in\n",
    "print(f'Support Vector Machine Score: {svc.score(test_TfIdf, test_y)}')\n",
    "print(f'Decision Tree Score: {Tree.score(test_TfIdf, test_y)}')\n",
    "print(f'Naive Bayes Score: {NB.score(test_TfIdf.toarray(), test_y)}')\n",
    "print(f'Logistic-regression: {regression.score(test_TfIdf, test_y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b9cb0d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classificaion Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.83      0.91      0.87       495\n",
      "    negative       0.90      0.81      0.85       505\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.86      0.86      0.86      1000\n",
      "weighted avg       0.86      0.86      0.86      1000\n",
      "\n",
      "\n",
      "Decision Tree Classificaion Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.67      0.66      0.67       495\n",
      "    negative       0.67      0.69      0.68       505\n",
      "\n",
      "    accuracy                           0.67      1000\n",
      "   macro avg       0.67      0.67      0.67      1000\n",
      "weighted avg       0.67      0.67      0.67      1000\n",
      "\n",
      "\n",
      "Naive Bayes Classificaion Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.60      0.62      0.61       495\n",
      "    negative       0.62      0.60      0.61       505\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.61      0.61      0.61      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "\n",
      "Logistic-regression Classificaion Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.83      0.91      0.87       495\n",
      "    negative       0.90      0.82      0.86       505\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.87      0.86      0.86      1000\n",
      "weighted avg       0.87      0.86      0.86      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can also get a little more info using the classification_report object from sklearn. \n",
    "print(\"SVM Classificaion Report\")\n",
    "print(classification_report(test_y, svc.predict(test_TfIdf), labels=['positive', 'negative']))\n",
    "\n",
    "print(\"\\nDecision Tree Classificaion Report\")\n",
    "print(classification_report(test_y, Tree.predict(test_TfIdf), labels=['positive', 'negative']))\n",
    "\n",
    "print(\"\\nNaive Bayes Classificaion Report\")\n",
    "print(classification_report(test_y, NB.predict(test_TfIdf.toarray()), labels=['positive', 'negative']))\n",
    "\n",
    "print(\"\\nLogistic-regression Classificaion Report\")\n",
    "print(classification_report(test_y, regression.predict(test_TfIdf), labels=['positive', 'negative']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00312730",
   "metadata": {},
   "source": [
    "Based on our results the SVM and logistic regression models are neck and neck for top performance. The decision tree and naive Bayes classifiers suffer alot more in comparison. This isn't too suprising since the latter two are data hungry for getting good predictive scores. You can also see though that these two models may be more desirable for large data set scenarios due to their fitting speed. The decision tree model was almost 7x faster at executing a fit compared to the SVM.\n",
    "\n",
    "Based on the score and speed though the logistic regression is the winner for our case. The speed of this model owes to the fact its only fitting a line using maximum likelihood as a cost function.\n",
    "\n",
    "Since we are only dealing with two classification options (pos/neg) the accuracy values are pretty representative of how well the models perform. However, we can get more information out of the classification report with respect to true positive rates (recall) and positive predictive values (precision). The f1-score also provides a harmonic mean between these two values to compare the performance of each model using one metric. \n",
    "\n",
    "From here we could grid search say the SVM model to fine tune the hyper parameters, or introduce some more NLP pre-processing steps if we weren't happy with the performance. This is kind of a general guide to how I would approach these types of problems though. Hope you enjoyed!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
